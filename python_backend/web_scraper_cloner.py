"""
Web Scraper Cloner - Tool Ä‘á»ƒ clone cÃ´ng thá»©c tá»« cÃ¡c website náº¥u Äƒn
Há»— trá»£ scrape tá»«: Cooky, VnExpress, RecipeTin, AllRecipes, v.v.
"""

import os
import sys
import json
import sqlite3
import requests
from bs4 import BeautifulSoup
from datetime import datetime
from typing import List, Dict, Any, Optional
import logging
from urllib.parse import urljoin, urlparse

# Cáº¥u hÃ¬nh logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class WebScraperCloner:
    """Tool Ä‘á»ƒ scrape cÃ´ng thá»©c tá»« web vÃ  thÃªm vÃ o database"""
    
    def __init__(self, db_path: str = "recipes.db"):
        """
        Khá»Ÿi táº¡o WebScraperCloner
        
        Args:
            db_path: ÄÆ°á»ng dáº«n Ä‘áº¿n database
        """
        self.db_path = db_path
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        self._init_db()
        logger.info(f"âœ… WebScraperCloner initialized with database: {db_path}")
    
    def _init_db(self):
        """Táº¡o tables náº¿u chÆ°a tá»“n táº¡i"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS recipes (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    title TEXT NOT NULL,
                    imageUrl TEXT,
                    description TEXT,
                    steps TEXT,
                    durationInMinutes INTEGER,
                    type TEXT,
                    source TEXT,
                    cloned_at TEXT
                )
            ''')
            
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS ingredients (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    recipeId INTEGER NOT NULL,
                    name TEXT NOT NULL,
                    isChecked INTEGER DEFAULT 0,
                    FOREIGN KEY(recipeId) REFERENCES recipes(id) ON DELETE CASCADE
                )
            ''')
            
            conn.commit()
            conn.close()
            logger.info("ğŸ“Š Database tables verified/created")
        except Exception as e:
            logger.error(f"âŒ Error initializing database: {e}")
            raise
    
    def _add_recipe(self, recipe_data: Dict[str, Any], source_url: str = "web_scrape") -> bool:
        """
        ThÃªm cÃ´ng thá»©c vÃ o database
        
        Args:
            recipe_data: Dá»¯ liá»‡u cÃ´ng thá»©c
            source_url: URL nguá»“n
            
        Returns:
            True náº¿u thÃªm thÃ nh cÃ´ng
        """
        try:
            if not recipe_data.get('title'):
                logger.warning(f"âš ï¸ Skipping recipe: missing title")
                return False
            
            title = recipe_data['title']
            description = recipe_data.get('description', '')
            duration = recipe_data.get('durationInMinutes', 30)
            recipe_type = recipe_data.get('type', 'KhÃ¡c')
            image_url = recipe_data.get('imageUrl', '')
            ingredients = recipe_data.get('ingredients', [])
            steps = recipe_data.get('steps', [])
            
            steps_json = json.dumps(steps, ensure_ascii=False) if steps else json.dumps([])
            
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute('SELECT id FROM recipes WHERE title = ?', (title,))
            if cursor.fetchone():
                logger.warning(f"âš ï¸ Recipe already exists: {title}")
                conn.close()
                return False
            
            cursor.execute('''
                INSERT INTO recipes 
                (title, imageUrl, description, steps, durationInMinutes, type, source, cloned_at)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                title, image_url, description, steps_json, duration, 
                recipe_type, source_url, datetime.now().isoformat()
            ))
            
            recipe_id = cursor.lastrowid
            
            for ingredient in ingredients:
                cursor.execute('''
                    INSERT INTO ingredients (recipeId, name, isChecked)
                    VALUES (?, ?, 0)
                ''', (recipe_id, ingredient))
            
            conn.commit()
            conn.close()
            
            logger.info(f"âœ… Recipe added: {title} (ID: {recipe_id}, {len(ingredients)} ingredients)")
            return True
            
        except Exception as e:
            logger.error(f"âŒ Error adding recipe: {e}")
            return False
    
    # ===== Scraper Cooky.vn =====
    
    def scrape_cooky(self, recipe_url: str) -> Optional[Dict]:
        """
        Scrape cÃ´ng thá»©c tá»« Cooky.vn
        
        Args:
            recipe_url: URL cá»§a cÃ´ng thá»©c (vd: https://cooky.vn/recipe/12345)
            
        Returns:
            Dict chá»©a dá»¯ liá»‡u cÃ´ng thá»©c
        """
        try:
            logger.info(f"ğŸŒ Scraping Cooky: {recipe_url}")
            
            response = self.session.get(recipe_url, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Láº¥y tiÃªu Ä‘á»
            title_elem = soup.find('h1', class_=['title', 'recipe-title'])
            title = title_elem.text.strip() if title_elem else "Unknown"
            
            # Láº¥y mÃ´ táº£
            desc_elem = soup.find(['meta[name="description"]', 'p', 'div'], class_=['description', 'intro'])
            description = desc_elem.get('content') if desc_elem.name == 'meta' else (
                desc_elem.text.strip() if desc_elem else ""
            )
            
            # Láº¥y hÃ¬nh áº£nh
            img_elem = soup.find('img', class_=['recipe-image', 'main-image'])
            image_url = img_elem.get('src', '') if img_elem else ''
            
            # Láº¥y thá»i gian
            time_elem = soup.find(['span', 'div'], class_=['time', 'duration', 'cook-time'])
            duration = 30  # Default
            if time_elem:
                time_text = time_elem.text.lower()
                for word in time_text.split():
                    if word.isdigit():
                        duration = int(word)
                        break
            
            # Láº¥y nguyÃªn liá»‡u
            ingredients = []
            ingredients_section = soup.find(['ul', 'ol', 'div'], class_=['ingredients', 'ingredient-list', 'ingredients-list'])
            if ingredients_section:
                for item in ingredients_section.find_all(['li', 'p', 'div'], class_=['ingredient', 'ingredient-item']):
                    text = item.text.strip()
                    if text and text not in ingredients:
                        ingredients.append(text)
            
            # Láº¥y cÃ¡c bÆ°á»›c
            steps = []
            steps_section = soup.find(['ol', 'div', 'ul'], class_=['steps', 'instructions', 'directions'])
            if steps_section:
                for item in steps_section.find_all(['li', 'p', 'div'], class_=['step', 'instruction', 'direction']):
                    text = item.text.strip()
                    if text and text not in steps:
                        steps.append(text)
            
            return {
                "title": title,
                "description": description[:200],  # Limit 200 chars
                "imageUrl": image_url,
                "durationInMinutes": duration,
                "type": "Viá»‡t Nam",
                "ingredients": ingredients[:20],  # Limit 20 ingredients
                "steps": steps[:15]  # Limit 15 steps
            }
            
        except Exception as e:
            logger.error(f"âŒ Error scraping Cooky: {e}")
            return None
    
    # ===== Scraper RecipeTin.com =====
    
    def scrape_recipetin(self, recipe_url: str) -> Optional[Dict]:
        """
        Scrape cÃ´ng thá»©c tá»« RecipeTin (há»— trá»£ recipe cÃ³ JSON-LD schema)
        
        Args:
            recipe_url: URL cá»§a cÃ´ng thá»©c
            
        Returns:
            Dict chá»©a dá»¯ liá»‡u cÃ´ng thá»©c
        """
        try:
            logger.info(f"ğŸŒ Scraping RecipeTin: {recipe_url}")
            
            response = self.session.get(recipe_url, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # TÃ¬m JSON-LD schema
            json_ld = soup.find('script', type='application/ld+json')
            if json_ld:
                try:
                    schema = json.loads(json_ld.string)
                    
                    title = schema.get('name', 'Unknown')
                    description = schema.get('description', '')[:200]
                    image_url = schema.get('image', {})
                    if isinstance(image_url, list):
                        image_url = image_url[0] if image_url else ''
                    
                    # Láº¥y thá»i gian
                    cook_time = schema.get('cookTime', 'PT30M')
                    duration = self._parse_iso_duration(cook_time)
                    
                    # Láº¥y nguyÃªn liá»‡u
                    ingredients = []
                    for ing in schema.get('recipeIngredient', []):
                        if ing and ing not in ingredients:
                            ingredients.append(ing)
                    
                    # Láº¥y cÃ¡c bÆ°á»›c
                    steps = []
                    for step_obj in schema.get('recipeInstructions', []):
                        if isinstance(step_obj, dict):
                            step_text = step_obj.get('text', '')
                        else:
                            step_text = str(step_obj)
                        if step_text and step_text not in steps:
                            steps.append(step_text)
                    
                    return {
                        "title": title,
                        "description": description,
                        "imageUrl": image_url,
                        "durationInMinutes": duration,
                        "type": "KhÃ¡c",
                        "ingredients": ingredients[:20],
                        "steps": steps[:15]
                    }
                
                except json.JSONDecodeError:
                    logger.warning("âš ï¸ Failed to parse JSON-LD schema")
            
            # Fallback: scrape HTML
            return self._scrape_html_fallback(soup, recipe_url)
            
        except Exception as e:
            logger.error(f"âŒ Error scraping RecipeTin: {e}")
            return None
    
    def _parse_iso_duration(self, duration_str: str) -> int:
        """
        Parse ISO 8601 duration string (vd: PT30M, PT1H30M)
        
        Args:
            duration_str: ISO duration string
            
        Returns:
            Sá»‘ phÃºt
        """
        try:
            minutes = 0
            
            # Remove 'PT'
            duration_str = duration_str.replace('PT', '').upper()
            
            # Parse hours
            if 'H' in duration_str:
                hours_str = duration_str.split('H')[0]
                if hours_str:
                    minutes += int(hours_str) * 60
                duration_str = duration_str.split('H')[1]
            
            # Parse minutes
            if 'M' in duration_str:
                min_str = duration_str.split('M')[0]
                if min_str:
                    minutes += int(min_str)
            
            return max(minutes, 30)  # Minimum 30 minutes
            
        except:
            return 30
    
    def _scrape_html_fallback(self, soup: BeautifulSoup, url: str) -> Dict:
        """Fallback HTML scraping"""
        
        title = "Unknown"
        title_elem = soup.find(['h1', 'h2'])
        if title_elem:
            title = title_elem.text.strip()
        
        description = ""
        desc_elem = soup.find('p', class_=['description', 'intro'])
        if desc_elem:
            description = desc_elem.text.strip()[:200]
        
        image_url = ""
        img_elem = soup.find('img')
        if img_elem:
            image_url = img_elem.get('src', '')
            if image_url and not image_url.startswith('http'):
                image_url = urljoin(url, image_url)
        
        return {
            "title": title,
            "description": description,
            "imageUrl": image_url,
            "durationInMinutes": 30,
            "type": "KhÃ¡c",
            "ingredients": [],
            "steps": []
        }
    
    # ===== Batch Scraper =====
    
    def scrape_multiple(self, urls: List[str], website: str = "auto") -> int:
        """
        Scrape nhiá»u URL
        
        Args:
            urls: Danh sÃ¡ch URLs
            website: Website (cooky, recipetin, auto)
            
        Returns:
            Sá»‘ cÃ´ng thá»©c Ä‘Æ°á»£c thÃªm thÃ nh cÃ´ng
        """
        logger.info(f"ğŸŒ Scraping {len(urls)} URLs from {website}")
        
        count = 0
        for i, url in enumerate(urls, 1):
            logger.info(f"\nğŸ“ [{i}/{len(urls)}] Processing: {url}")
            
            recipe = None
            
            if website == "cooky" or (website == "auto" and "cooky.vn" in url):
                recipe = self.scrape_cooky(url)
            elif website == "recipetin" or (website == "auto" and ("recipetin" in url or "allrecipes" in url)):
                recipe = self.scrape_recipetin(url)
            else:
                recipe = self.scrape_recipetin(url)  # Try default
            
            if recipe and self._add_recipe(recipe, source_url=website):
                count += 1
        
        logger.info(f"\nâœ… Successfully scraped and added {count}/{len(urls)} recipes")
        return count
    
    def get_statistics(self) -> Dict:
        """Láº¥y thá»‘ng kÃª"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute('SELECT COUNT(*) FROM recipes')
            total_recipes = cursor.fetchone()[0]
            
            cursor.execute('''
                SELECT source, COUNT(*) as count 
                FROM recipes 
                GROUP BY source
            ''')
            by_source = {row[0]: row[1] for row in cursor.fetchall()}
            
            cursor.execute('SELECT COUNT(*) FROM ingredients')
            total_ingredients = cursor.fetchone()[0]
            
            conn.close()
            
            return {
                "total_recipes": total_recipes,
                "by_source": by_source,
                "total_ingredients": total_ingredients
            }
            
        except Exception as e:
            logger.error(f"âŒ Error getting statistics: {e}")
            return {}


# ===== CLI Tool =====

def main():
    """CLI interface cho WebScraperCloner"""
    
    print("""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘   ğŸ•·ï¸  WEB SCRAPER CLONER TOOL          â•‘
    â•‘      Clone cÃ´ng thá»©c tá»« web            â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    scraper = WebScraperCloner()
    
    while True:
        print("\nğŸ“‹ Menu:")
        print("1. ğŸ•·ï¸  Scrape tá»« URL riÃªng láº»")
        print("2. ğŸ“‹ Scrape tá»« danh sÃ¡ch URLs")
        print("3. ğŸ“Š Xem thá»‘ng kÃª")
        print("0. âŒ ThoÃ¡t")
        
        choice = input("\nğŸ‘‰ Chá»n (0-3): ").strip()
        
        if choice == '1':
            url = input("Nháº­p URL cÃ´ng thá»©c: ").strip()
            if not url.startswith('http'):
                url = 'https://' + url
            
            website = input("Website (cooky/recipetin/auto) [auto]: ").strip() or "auto"
            
            recipe = None
            if "cooky.vn" in url:
                recipe = scraper.scrape_cooky(url)
            else:
                recipe = scraper.scrape_recipetin(url)
            
            if recipe:
                if scraper._add_recipe(recipe, source_url=website):
                    print("\nâœ… CÃ´ng thá»©c Ä‘Ã£ Ä‘Æ°á»£c thÃªm")
                    print(f"   TiÃªu Ä‘á»: {recipe['title']}")
                    print(f"   NguyÃªn liá»‡u: {len(recipe['ingredients'])}")
                    print(f"   BÆ°á»›c: {len(recipe['steps'])}")
                else:
                    print("\nâš ï¸ CÃ´ng thá»©c cÃ³ thá»ƒ Ä‘Ã£ tá»“n táº¡i")
            else:
                print("\nâŒ Lá»—i khi scrape URL")
        
        elif choice == '2':
            url_file = input("Nháº­p Ä‘Æ°á»ng dáº«n file chá»©a danh sÃ¡ch URLs: ").strip()
            
            if not os.path.exists(url_file):
                print(f"âŒ File khÃ´ng tÃ¬m tháº¥y: {url_file}")
                continue
            
            try:
                with open(url_file, 'r', encoding='utf-8') as f:
                    urls = [line.strip() for line in f if line.strip() and line.startswith('http')]
                
                if not urls:
                    print("âŒ KhÃ´ng cÃ³ URLs há»£p lá»‡ trong file")
                    continue
                
                website = input("Website (cooky/recipetin/auto) [auto]: ").strip() or "auto"
                
                count = scraper.scrape_multiple(urls, website)
                
                stats = scraper.get_statistics()
                print(f"\nğŸ“Š Thá»‘ng kÃª:")
                print(f"   Tá»•ng cÃ´ng thá»©c: {stats['total_recipes']}")
                print(f"   Tá»•ng nguyÃªn liá»‡u: {stats['total_ingredients']}")
                
            except Exception as e:
                print(f"âŒ Lá»—i: {e}")
        
        elif choice == '3':
            stats = scraper.get_statistics()
            print(f"\nğŸ“Š Thá»‘ng kÃª:")
            print(f"   Tá»•ng cÃ´ng thá»©c: {stats['total_recipes']}")
            print(f"   Tá»•ng nguyÃªn liá»‡u: {stats['total_ingredients']}")
            print(f"\n   Theo nguá»“n:")
            for source, count in stats.get('by_source', {}).items():
                print(f"      - {source}: {count}")
        
        elif choice == '0':
            print("\nğŸ‘‹ Táº¡m biá»‡t!")
            break
        
        else:
            print("âŒ Lá»±a chá»n khÃ´ng há»£p lá»‡")


if __name__ == "__main__":
    main()
